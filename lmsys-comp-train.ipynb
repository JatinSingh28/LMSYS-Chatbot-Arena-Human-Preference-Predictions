{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[],"source":["# !pip install  -q kaggle kagglehub\n","# !kaggle datasets download -d raedmughaus/bitsandbytes-0-42-0-py3-none-any-whl\n","# # !kaggle datasets download -d jatinsinghsagoi/lmsys-200k-trainig-data\n","# !kaggle competitions download -c lmsys-chatbot-arena"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# !unzip /teamspace/studios/this_studio/bitsandbytes-0-42-0-py3-none-any-whl.zip\n","# !unzip /teamspace/studios/this_studio/lmsys-chatbot-arena"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[],"source":["# import kagglehub\n","\n","# kagglehub.model_download(\"jatinsinghsagoi/peft-wheel/pyTorch/version1\")\n","# kagglehub.model_download(\"metaresearch/llama-3/transformers/8b-hf\")"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[],"source":["# !pip install -U bitsandbytes-0.42.0-py3-none-any.whl -qq\n","# !pip install -U peft-wheel/pyTorch/version1/1/peft-0.10.0-py3-none-any.whl -qq\n","# # !pip install -q trl accelerate\n","# !pip install -q -U git+https://github.com/huggingface/accelerate.git\n","# # !pip install -i https://pypi.org/simple/ bitsandbytes\n","# !pip install -q transformers accelerate peft datasets bitsandbytes torch"]},{"cell_type":"code","execution_count":5,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import pandas as pd\n","from datasets import Dataset\n","import wandb\n","import torch\n","from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig\n","from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n","import torch.nn as nn\n","import transformers\n","from sklearn.metrics import log_loss\n","import torch\n","from torch.utils.data import DataLoader\n","from transformers import AdamW, get_scheduler\n","import torch.nn.functional as F\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","DEBUG = False"]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(10, 9)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>model_a</th>\n","      <th>model_b</th>\n","      <th>prompt</th>\n","      <th>response_a</th>\n","      <th>response_b</th>\n","      <th>winner_model_a</th>\n","      <th>winner_model_b</th>\n","      <th>winner_tie</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>30192</td>\n","      <td>gpt-4-1106-preview</td>\n","      <td>gpt-4-0613</td>\n","      <td>[\"Is it morally right to try to have a certain...</td>\n","      <td>[\"The question of whether it is morally right ...</td>\n","      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>53567</td>\n","      <td>koala-13b</td>\n","      <td>gpt-4-0613</td>\n","      <td>[\"What is the difference between marriage lice...</td>\n","      <td>[\"A marriage license is a legal document that ...</td>\n","      <td>[\"A marriage license and a marriage certificat...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>65089</td>\n","      <td>gpt-3.5-turbo-0613</td>\n","      <td>mistral-medium</td>\n","      <td>[\"explain function calling. how would you call...</td>\n","      <td>[\"Function calling is the process of invoking ...</td>\n","      <td>[\"Function calling is the process of invoking ...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>96401</td>\n","      <td>llama-2-13b-chat</td>\n","      <td>mistral-7b-instruct</td>\n","      <td>[\"How can I create a test set for a very rare ...</td>\n","      <td>[\"Creating a test set for a very rare category...</td>\n","      <td>[\"When building a classifier for a very rare c...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>198779</td>\n","      <td>koala-13b</td>\n","      <td>gpt-3.5-turbo-0314</td>\n","      <td>[\"What is the best way to travel from Tel-Aviv...</td>\n","      <td>[\"The best way to travel from Tel Aviv to Jeru...</td>\n","      <td>[\"The best way to travel from Tel-Aviv to Jeru...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       id             model_a              model_b  \\\n","0   30192  gpt-4-1106-preview           gpt-4-0613   \n","1   53567           koala-13b           gpt-4-0613   \n","2   65089  gpt-3.5-turbo-0613       mistral-medium   \n","3   96401    llama-2-13b-chat  mistral-7b-instruct   \n","4  198779           koala-13b   gpt-3.5-turbo-0314   \n","\n","                                              prompt  \\\n","0  [\"Is it morally right to try to have a certain...   \n","1  [\"What is the difference between marriage lice...   \n","2  [\"explain function calling. how would you call...   \n","3  [\"How can I create a test set for a very rare ...   \n","4  [\"What is the best way to travel from Tel-Aviv...   \n","\n","                                          response_a  \\\n","0  [\"The question of whether it is morally right ...   \n","1  [\"A marriage license is a legal document that ...   \n","2  [\"Function calling is the process of invoking ...   \n","3  [\"Creating a test set for a very rare category...   \n","4  [\"The best way to travel from Tel Aviv to Jeru...   \n","\n","                                          response_b  winner_model_a  \\\n","0  [\"As an AI, I don't have personal beliefs or o...               1   \n","1  [\"A marriage license and a marriage certificat...               0   \n","2  [\"Function calling is the process of invoking ...               0   \n","3  [\"When building a classifier for a very rare c...               1   \n","4  [\"The best way to travel from Tel-Aviv to Jeru...               0   \n","\n","   winner_model_b  winner_tie  \n","0               0           0  \n","1               1           0  \n","2               0           1  \n","3               0           0  \n","4               1           0  "]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["if DEBUG:\n","    df = pd.read_csv(\"train.csv\",nrows=10)\n","else:\n","    df = pd.read_csv(\"train.csv\")\n","print(df.shape)\n","df.head()"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>model_a</th>\n","      <th>model_b</th>\n","      <th>prompt</th>\n","      <th>response_a</th>\n","      <th>response_b</th>\n","      <th>winner_model_a</th>\n","      <th>winner_model_b</th>\n","      <th>winner_tie</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>441448</td>\n","      <td>gpt-3.5-turbo-0613</td>\n","      <td>vicuna-13b</td>\n","      <td>\"translate to russian the followig sentence  B...</td>\n","      <td>\"\\u0411\\u043e\\u043b\\u044c\\u0448\\u0438\\u0435 \\u...</td>\n","      <td>\"\\u0411\\u043e\\u043b\\u044c\\u0448\\u0438\\u0435 \\u...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>53567</td>\n","      <td>koala-13b</td>\n","      <td>gpt-4-0613</td>\n","      <td>\"What is the difference between marriage licen...</td>\n","      <td>\"A marriage license is a legal document that a...</td>\n","      <td>\"A marriage license and a marriage certificate...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>292873</td>\n","      <td>vicuna-13b</td>\n","      <td>gpt-4-0314</td>\n","      <td>\"Construct a rap battle, in the style of Epic ...</td>\n","      <td>\"[Zeus]\\nYo, it's the king of the gods on the ...</td>\n","      <td>\"(Verse 1 - Zeus)\\n\\nI'm the king of the gods,...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>30192</td>\n","      <td>gpt-4-1106-preview</td>\n","      <td>gpt-4-0613</td>\n","      <td>\"Is it morally right to try to have a certain ...</td>\n","      <td>\"The question of whether it is morally right t...</td>\n","      <td>\"As an AI, I don't have personal beliefs or op...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>370945</td>\n","      <td>gemini-pro</td>\n","      <td>claude-2.0</td>\n","      <td>\"\\\"Bacteria is life on Mars but a heartbeat is...</td>\n","      <td>\"Dune\"</td>\n","      <td>\"This quote seems to be referencing the debate...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       id             model_a     model_b  \\\n","0  441448  gpt-3.5-turbo-0613  vicuna-13b   \n","1   53567           koala-13b  gpt-4-0613   \n","2  292873          vicuna-13b  gpt-4-0314   \n","3   30192  gpt-4-1106-preview  gpt-4-0613   \n","4  370945          gemini-pro  claude-2.0   \n","\n","                                              prompt  \\\n","0  \"translate to russian the followig sentence  B...   \n","1  \"What is the difference between marriage licen...   \n","2  \"Construct a rap battle, in the style of Epic ...   \n","3  \"Is it morally right to try to have a certain ...   \n","4  \"\\\"Bacteria is life on Mars but a heartbeat is...   \n","\n","                                          response_a  \\\n","0  \"\\u0411\\u043e\\u043b\\u044c\\u0448\\u0438\\u0435 \\u...   \n","1  \"A marriage license is a legal document that a...   \n","2  \"[Zeus]\\nYo, it's the king of the gods on the ...   \n","3  \"The question of whether it is morally right t...   \n","4                                             \"Dune\"   \n","\n","                                          response_b  winner_model_a  \\\n","0  \"\\u0411\\u043e\\u043b\\u044c\\u0448\\u0438\\u0435 \\u...               0   \n","1  \"A marriage license and a marriage certificate...               0   \n","2  \"(Verse 1 - Zeus)\\n\\nI'm the king of the gods,...               0   \n","3  \"As an AI, I don't have personal beliefs or op...               1   \n","4  \"This quote seems to be referencing the debate...               0   \n","\n","   winner_model_b  winner_tie  \n","0               1           0  \n","1               1           0  \n","2               1           0  \n","3               0           0  \n","4               1           0  "]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["def strip(row):\n","    return row.strip('[]')\n","df['prompt'] = df['prompt'].apply(strip)\n","df['response_a'] = df['response_a'].apply(strip)\n","df['response_b'] = df['response_b'].apply(strip)\n","\n","df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n","df.head()"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[],"source":["config = {\n","    \"model_name\": \"llama-3/transformers/8b-hf/1\",\n","    \"max_length\": 1284,\n","    \"batch_size\": 8,\n","    \"r\": 16,\n","    \"lora_alpha\": 32,\n","    \"lora_dropout\": 0.10,\n","    \"lr\": 2e-5,\n","    \"num_epochs\": 1,\n","    \"num_warmup_steps\": 100\n","}\n"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjatinsingh\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /teamspace/studios/this_studio/.netrc\n"]},{"data":{"text/html":["Tracking run with wandb version 0.17.4"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/teamspace/studios/this_studio/wandb/run-20240707_071528-ybgp23rr</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/jatinsingh/LMSYS/runs/ybgp23rr' target=\"_blank\">LMSYS train data</a></strong> to <a href='https://wandb.ai/jatinsingh/LMSYS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/jatinsingh/LMSYS' target=\"_blank\">https://wandb.ai/jatinsingh/LMSYS</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/jatinsingh/LMSYS/runs/ybgp23rr' target=\"_blank\">https://wandb.ai/jatinsingh/LMSYS/runs/ybgp23rr</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/jatinsingh/LMSYS/runs/ybgp23rr?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7f8bfbc837c0>"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# from kaggle_secrets import UserSecretsClient\n","\n","# user_secrets = UserSecretsClient()\n","\n","# my_secret = user_secrets.get_secret(\"wandb\") \n","\n","wandb.login(key=\"3f6cbde2c69fb457986fc88500866ae2893e2c87\")\n","wandb.init(project=\"LMSYS\",name = \"LMSYS train data\", config=config)"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[],"source":["df['labels'] = df[['winner_model_a','winner_model_b','winner_tie']].values.tolist()\n","df['prompt'] = 'User prompt: ' + df['prompt'] +  '\\n\\nModel A :\\n' + df['response_a'] +'\\n\\n-----------\\n\\nModel B:\\n'  + df['response_b']"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[],"source":["data = Dataset.from_pandas(df[['prompt','labels']])"]},{"cell_type":"code","execution_count":13,"metadata":{"trusted":true},"outputs":[],"source":["split_data = data.train_test_split(test_size=.2)\n","train_data = split_data['train']\n","eval_data = split_data['test']"]},{"cell_type":"code","execution_count":14,"metadata":{"trusted":true},"outputs":[],"source":["torch.backends.cuda.enable_mem_efficient_sdp(False)\n","torch.backends.cuda.enable_flash_sdp(False)"]},{"cell_type":"code","execution_count":15,"metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e5a96bccf6de40859994b1fcf73b851c","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at llama-3/transformers/8b-hf/1 and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["MODEL_NAME = config.get('model_name')\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","\n","model = LlamaForSequenceClassification.from_pretrained(\n","    MODEL_NAME,\n","    num_labels=3,\n","    device_map=\"auto\",\n","    quantization_config=bnb_config\n",")\n","model = prepare_model_for_kbit_training(model)\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","tokenizer.pad_token_id = tokenizer.eos_token_id\n","tokenizer.pad_token = tokenizer.eos_token"]},{"cell_type":"code","execution_count":16,"metadata":{"trusted":true},"outputs":[],"source":["lora_config = LoraConfig(\n","    r=config.get('r'),\n","    lora_alpha=config.get('lora_alpha'),\n","    lora_dropout=0.10,\n","    bias='none',\n","    task_type=TaskType.SEQ_CLS,\n","    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",")\n","\n","model = get_peft_model(model, lora_config)\n","model.config.pad_token_id = tokenizer.pad_token_id\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1"]},{"cell_type":"code","execution_count":17,"metadata":{"trusted":true},"outputs":[],"source":["def tokenize_function(examples):\n","    return tokenizer(examples['prompt'], padding=\"max_length\", truncation=True, max_length = config.get('max_length'))"]},{"cell_type":"code","execution_count":18,"metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"32fd66b434dd46668f730cb454638df7","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/8 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"113791558c1748c3962edb8632b77e31","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/2 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["train_dataset = train_data.map(tokenize_function, batched=True)\n","eval_dataset = eval_data.map(tokenize_function, batched=True)\n","train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","eval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":19,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}],"source":["model.to(device)\n","optimizer = AdamW(model.parameters(), config.get('lr'))\n","num_epochs = config.get('num_epochs')\n","\n","train_loader = DataLoader(train_dataset, batch_size=config.get('batch_size'), shuffle=True)\n","eval_loader = DataLoader(eval_dataset, batch_size=config.get('batch_size'), shuffle=False)\n","\n","num_training_steps = num_epochs * len(train_loader)\n","lr_scheduler = get_scheduler(\n","    name=\"linear\", optimizer=optimizer, num_warmup_steps=config.get('num_warmup_steps'), num_training_steps=num_training_steps\n",")\n"]},{"cell_type":"code","execution_count":20,"metadata":{"trusted":true},"outputs":[],"source":["def compute_loss(logits, labels):\n","    logits = logits.cpu().detach().numpy()\n","    labels = labels.cpu().detach().numpy()\n","#     logits = softmax(logits, axis=1)\n","    logits = F.softmax(torch.tensor(logits), dim=1).numpy()\n","    log_loss_value = log_loss(labels, logits)\n","    log_loss_value_tensor = torch.tensor(log_loss_value, requires_grad=True)\n","    \n","    return log_loss_value_tensor"]},{"cell_type":"code","execution_count":21,"metadata":{"trusted":true},"outputs":[],"source":["def train_epoch(model, dataloader, optimizer, device, lr_scheduler):\n","    model.train()\n","    total_loss = 0\n","    progress_bar = tqdm(dataloader, desc=f\"Train Epoch {epoch+1}\")\n","    for batch in progress_bar:\n","        optimizer.zero_grad()\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        loss = compute_loss(logits, labels)\n","        loss.backward()\n","        optimizer.step()\n","        lr_scheduler.step()\n","\n","        total_loss += loss.item()\n","    avg_loss = total_loss / len(dataloader)\n","    wandb.log({\"Train Loss\": avg_loss, \"Epoch\": epoch})\n","    return avg_loss\n","\n","def evaluate(model, dataloader, device):\n","    model.eval()\n","    total_loss = 0\n","    with torch.no_grad():\n","        progress_bar = tqdm(dataloader, desc=f\"Eval Epoch {epoch+1}\")\n","        for batch in progress_bar:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels'].to(device)\n","\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","            logits = outputs.logits\n","            loss = compute_loss(logits, labels)\n","            total_loss += loss.item()\n","    avg_loss = total_loss / len(dataloader)\n","    wandb.log({\"Val Loss\": avg_loss, \"Epoch\": epoch})\n","    return avg_loss\n"]},{"cell_type":"code","execution_count":22,"metadata":{"trusted":true},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":23,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/1\n"]},{"name":"stderr","output_type":"stream","text":["Train Epoch 1: 100%|██████████| 1/1 [00:10<00:00, 10.35s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Train loss: 2.3151\n"]},{"name":"stderr","output_type":"stream","text":["Eval Epoch 1: 100%|██████████| 1/1 [00:02<00:00,  2.56s/it]"]},{"name":"stdout","output_type":"stream","text":["Eval loss: 0.8451\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Early stopping parameters\n","patience = 1\n","best_eval_loss = float('inf')\n","epochs_without_improvement = 0\n","\n","for epoch in range(num_epochs):\n","    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n","    train_loss = train_epoch(model, train_loader, optimizer, device, lr_scheduler)\n","    print(f\"Train loss: {train_loss:.4f}\")\n","    eval_loss = evaluate(model, eval_loader, device)\n","    print(f\"Eval loss: {eval_loss:.4f}\")\n","    \n","    if eval_loss < best_eval_loss:\n","        best_eval_loss = eval_loss\n","        epochs_without_improvement = 0\n","        model.save_pretrained(\"best-trained-lora\")        \n","    else:\n","        epochs_without_improvement += 1\n","        \n","    if epochs_without_improvement >= patience:\n","        print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n","        break"]},{"cell_type":"code","execution_count":24,"metadata":{"trusted":true},"outputs":[],"source":["torch.save(model.state_dict(), 'final_model.pt')\n","model.save_pretrained(\"trained-model-lora\")"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"text/plain":["<Artifact lora_model>"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["artifact = wandb.Artifact('lora_model', type='model')\n","artifact.add_file(\"/teamspace/studios/this_studio/trained-model-lora/adapter_config.json\")\n","artifact.add_file(\"/teamspace/studios/this_studio/trained-model-lora/adapter_model.safetensors\")\n","artifact.add_file(\"/teamspace/studios/this_studio/trained-model-lora/README.md\")\n","\n","wandb.log_artifact(artifact)"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["# !pip install dagshub"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as JatinSingh28\n","</pre>\n"],"text/plain":["Accessing as JatinSingh28\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Client created. Use the name of the repo <span style=\"font-weight: bold\">(</span>LMSYS-lora-finetune-for-llm-ranking<span style=\"font-weight: bold\">)</span> as the name of the bucket\n","</pre>\n"],"text/plain":["Client created. Use the name of the repo \u001b[1m(\u001b[0mLMSYS-lora-finetune-for-llm-ranking\u001b[1m)\u001b[0m as the name of the bucket\n"]},"metadata":{},"output_type":"display_data"}],"source":["from dagshub import get_repo_bucket_client\n","s3 = get_repo_bucket_client(\"JatinSingh28/LMSYS-lora-finetune-for-llm-ranking\")"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["import os"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["\n","base_path = \"trained-model-lora\"\n","for path in os.listdir(base_path):\n","    try:\n","        s3.upload_file(\n","                Filename=os.path.join(base_path,path),\n","                Bucket=\"LMSYS-lora-finetune-for-llm-ranking\",\n","                Key=\"lora/\"+path\n","            )\n","    except Exception as e:\n","        print(f\"Couldn't upload model to dagshub. Error: {e}\")"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["base_path = \"best-trained-lora\"\n","for path in os.listdir(base_path):\n","    try:\n","        s3.upload_file(\n","                Filename=os.path.join(base_path,path),\n","                Bucket=\"LMSYS-lora-finetune-for-llm-ranking\",\n","                Key=\"best-lora/\"+path\n","            )\n","    except Exception as e:\n","        print(f\"Couldn't upload model to dagshub. Error: {e}\")"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# s3.download_file(\n","#     Bucket=\"LMSYS-lora-finetune-for-llm-ranking\",  \n","#     Key=\"lora/README.md\",  \n","#     Filename=\"downloads/README.md\",  \n","# )\n","# s3.download_file(\n","#     Bucket=\"LMSYS-lora-finetune-for-llm-ranking\",  \n","#     Key=\"lora/adapter_config.json\",  \n","#     Filename=\"downloads/adapter_config.json\",  \n","# )\n","# s3.download_file(\n","#     Bucket=\"LMSYS-lora-finetune-for-llm-ranking\",  \n","#     Key=\"lora/adapter_model.safetensors\",  \n","#     Filename=\"downloads/adapter_model.safetensors\",  \n","# )"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"274d54114a6645c38415a4fb89f5aa0d","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='52.089 MB of 52.089 MB uploaded (0.005 MB deduped)\\r'), FloatProgress(value=1.0, m…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁</td></tr><tr><td>Train Loss</td><td>▁</td></tr><tr><td>Val Loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>0</td></tr><tr><td>Train Loss</td><td>2.31506</td></tr><tr><td>Val Loss</td><td>0.8451</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">LMSYS train data</strong> at: <a href='https://wandb.ai/jatinsingh/LMSYS/runs/ybgp23rr' target=\"_blank\">https://wandb.ai/jatinsingh/LMSYS/runs/ybgp23rr</a><br/> View project at: <a href='https://wandb.ai/jatinsingh/LMSYS' target=\"_blank\">https://wandb.ai/jatinsingh/LMSYS</a><br/>Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240707_071528-ybgp23rr/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["wandb.finish()"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4281572,"sourceId":7369493,"sourceType":"datasetVersion"},{"datasetId":5343414,"sourceId":8877199,"sourceType":"datasetVersion"},{"datasetId":5343502,"sourceId":8877472,"sourceType":"datasetVersion"},{"modelInstanceId":28071,"sourceId":33534,"sourceType":"modelInstanceVersion"},{"modelInstanceId":28079,"sourceId":33547,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
